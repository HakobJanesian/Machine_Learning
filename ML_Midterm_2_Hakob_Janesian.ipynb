{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d730ac4a",
   "metadata": {},
   "source": [
    "My ID - A09190061, last two digits - 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "837dd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "from sklearn.discriminant_analysis import  QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Rest of required libraries\n",
    "\n",
    "from sklearn.metrics import classification_report, plot_roc_curve, confusion_matrix, auc \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, mean_squared_error, r2_score\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.datasets import load_digits, make_friedman1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, auc, classification_report, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e9b3c",
   "metadata": {},
   "source": [
    "# Problem1: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7df40dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df1 = load_digits()\n",
    "X = pd.DataFrame(df1[\"data\"])\n",
    "y = df1[\"target\"]\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eab05615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "scaler = MinMaxScaler()\n",
    "# Pipeline for AdaBoost\n",
    "combined_model_adaboost = make_pipeline(scaler, AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492ce82",
   "metadata": {},
   "source": [
    "#### The computation power of my laptop is too low, so I used HalvingGridSearchCV instead of regular GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d85621a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('adaboostclassifier',\n",
       "                                               AdaBoostClassifier())]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={'adaboostclassifier__learning_rate': array([1.00000000e-05, 1.57904211e-01, 3.15798421e-01, 4.73692632e-01,\n",
       "       6.31586842e-01, 7.89481053e-01, 9.47375263e-01, 1.10526947e+00,\n",
       "       1.26316368e+00, 1.42105789e+00, 1.57895211e+00, 1.73684632e+00,\n",
       "       1.89474053e+00, 2.05263474e+00, 2.21052895e+00, 2.36842316e+00,\n",
       "       2.52631737e+00, 2.68421158e+00, 2.84210579e+00, 3.00000000e+00]),\n",
       "                                'adaboostclassifier__n_estimators': array([ 50, 100, 150, 200, 250, 300, 350])},\n",
       "                    refit=<function _refit_callable at 0x000001CFCFC31040>,\n",
       "                    scoring='f1_macro')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning for Adaboost\n",
    "n_estimators_grid = np.arange(50, 400, 50)\n",
    "learning_rate_grid = np.linspace(0.00001, 3, 20)\n",
    "optimal_model_adaboost = HalvingGridSearchCV(combined_model_adaboost, param_grid={\"adaboostclassifier__n_estimators\": n_estimators_grid, \"adaboostclassifier__learning_rate\": learning_rate_grid}, cv = 5, scoring='f1_macro', n_jobs=-1)\n",
    "optimal_model_adaboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1c24183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'adaboostclassifier__learning_rate': 2.368423157894737, 'adaboostclassifier__n_estimators': 350}\n",
      "Best F1 score: 0.8783942748458424\n"
     ]
    }
   ],
   "source": [
    "# Obtain the optimal model's best parameters and score\n",
    "best_params = optimal_model_adaboost.best_params_\n",
    "best_score = optimal_model_adaboost.best_score_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best F1 score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3c5b5",
   "metadata": {},
   "source": [
    "#### After the GridSearch we see that the optimal model chose learning rate = 2.368, and nestimators = 350, and the Best F1 score is 0.878(not to depend only on precision or recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f255ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_adaboost = optimal_model_adaboost.best_estimator_.predict_proba(X_test)\n",
    "precision_adaboost, recall_adaboost, thresholds_adaboost = precision_recall_curve(y_true = y_test, probas_pred = pred_prob_adaboost[:,1], pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba67eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting preciison-recall curves\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "plt.figure(figsize = (15,10)) \n",
    "plt.plot(recall_adaboost, precision_adaboost, label=\"Adaboost\") \n",
    "plt.plot(x,-x+1,\".\", markersize = 1.6) \n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22ba8b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Adaboost: 0.94857267087113\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC of Adaboost: {auc(recall_adaboost, precision_adaboost)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0555bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        38\n",
      "           1       0.97      0.80      0.88        41\n",
      "           2       0.96      0.76      0.85        34\n",
      "           3       0.91      0.69      0.78        29\n",
      "           4       0.92      0.89      0.90        37\n",
      "           5       0.86      0.94      0.90        33\n",
      "           6       0.97      0.97      0.97        36\n",
      "           7       0.92      0.94      0.93        35\n",
      "           8       0.69      0.95      0.80        39\n",
      "           9       0.81      0.92      0.86        38\n",
      "\n",
      "    accuracy                           0.89       360\n",
      "   macro avg       0.90      0.88      0.89       360\n",
      "weighted avg       0.90      0.89      0.89       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = optimal_model_adaboost.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"AdaBoost Classifier - Classification Report\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6ae8a",
   "metadata": {},
   "source": [
    "#### We see that for test data the classification report shows that the Accuracy is 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7abe913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.906666959169564\n",
      "CV score: 0.8783942748458424\n",
      "Test score: 0.8854102171141724\n"
     ]
    }
   ],
   "source": [
    "# Train score\n",
    "train_score = optimal_model_adaboost.score(X_train, y_train)\n",
    "print(f\"Train score: {train_score}\")\n",
    "\n",
    "# CV score (best_score_ attribute already contains this value)\n",
    "cv_score = best_score\n",
    "print(f\"CV score: {cv_score}\")\n",
    "\n",
    "# Test score\n",
    "test_score = optimal_model_adaboost.score(X_test, y_test)\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386c835",
   "metadata": {},
   "source": [
    "#### Discussion1: I initially claim that if my model's train score is greater than 0.85 then I will say there is no problem of overfitting. In this case the train score is equal to 0.9098 which is greater than my threshhold of underfitting then I claim that here this model has no issue with underfitting. Now let's move on and compare Train and CV scores. 0.9066 and 0.878 are not significantly far from each other, because I claim that if their difference less than 0.05 then the difference is not significant. As the Traina and CV scores are close to each other we can claim that there is no issue of overfitting. Now let's compare Test and CV scores.  0.885 and 0.878 are very very close to each other (less than the threshhold 0.05), so we can claim that the CV and Test sets are coming from the same distribution, so we are confident that the we have well tuned values for our hyperparams. Ultimatelly based on the abovementioned facts I claim that this model(Adaboost classifier) is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f40848",
   "metadata": {},
   "source": [
    "## Now Let's look to Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8c8dd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('gradientboostingclassifier',\n",
       "                                               GradientBoostingClassifier())]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={'gradientboostingclassifier__learning_rate': [0.001,\n",
       "                                                                              0.01,\n",
       "                                                                              0.1,\n",
       "                                                                              1,\n",
       "                                                                              2],\n",
       "                                'gradientboostingclassifier__n_estimators': [50,\n",
       "                                                                             100,\n",
       "                                                                             150,\n",
       "                                                                             200,\n",
       "                                                                             250,\n",
       "                                                                             300,\n",
       "                                                                             350]},\n",
       "                    refit=<function _refit_callable at 0x000001CFCFC31040>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the pipeline for GradientBoostingClassifier\n",
    "combined_model_gradientboosting = make_pipeline(scaler, GradientBoostingClassifier())\n",
    "\n",
    "param_grid = {\n",
    "    'gradientboostingclassifier__n_estimators': [50, 100, 150, 200, 250, 300, 350],\n",
    "    'gradientboostingclassifier__learning_rate': [0.001, 0.01, 0.1, 1, 2]\n",
    "}\n",
    "\n",
    "# Now let's perform the grid search to find the best hyperparameters\n",
    "optimal_model_gradboost = HalvingGridSearchCV(combined_model_gradientboosting, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "optimal_model_gradboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4670632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'gradientboostingclassifier__learning_rate': 1, 'gradientboostingclassifier__n_estimators': 200}\n",
      "Best F1 score: 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "# Obtain the optimal model's best parameters and score\n",
    "best_params = optimal_model_gradboost.best_params_\n",
    "best_score = optimal_model_gradboost.best_score_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best F1 score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ed55a",
   "metadata": {},
   "source": [
    "#### We see that the learning rate is equal to 1, nestimators is 200, and f1 score is 0.925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42c97c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_gradboost = optimal_model_gradboost.best_estimator_.predict_proba(X_test)\n",
    "precision_gradboost, recall_gradboost, thresholds_gradboost = precision_recall_curve(y_true = y_test, probas_pred = pred_prob_gradboost[:,1], pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "092b23b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting preciison-recall curves\n",
    "\n",
    "x = np.linspace(0,1,100)\n",
    "plt.figure(figsize = (15,10)) \n",
    "plt.plot(recall_gradboost, precision_gradboost, label=\"Gradient boosting\") \n",
    "plt.plot(x,-x+1,\".\", markersize = 1.6) \n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d36e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Gradient boosting classification: 0.9964052499567548\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC of Gradient boosting classification: {auc(recall_gradboost, precision_gradboost)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff6b5714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        38\n",
      "           1       1.00      0.98      0.99        41\n",
      "           2       1.00      0.94      0.97        34\n",
      "           3       0.96      0.90      0.93        29\n",
      "           4       0.95      0.95      0.95        37\n",
      "           5       0.97      1.00      0.99        33\n",
      "           6       0.95      0.97      0.96        36\n",
      "           7       0.92      0.97      0.94        35\n",
      "           8       0.93      0.95      0.94        39\n",
      "           9       0.92      0.92      0.92        38\n",
      "\n",
      "    accuracy                           0.95       360\n",
      "   macro avg       0.95      0.95      0.95       360\n",
      "weighted avg       0.95      0.95      0.95       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = optimal_model_gradboost.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"AdaBoost Classifier - Classification Report\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We see that for test data the classification report shows that the Accuracy is 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79b8e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9986082115518441\n",
      "CV score: 0.9259259259259259\n",
      "Test score: 0.9527777777777777\n"
     ]
    }
   ],
   "source": [
    "# Train score\n",
    "train_score = optimal_model_gradboost.score(X_train, y_train)\n",
    "print(f\"Train score: {train_score}\")\n",
    "\n",
    "# CV score (best_score_ attribute already contains this value)\n",
    "cv_score = best_score\n",
    "print(f\"CV score: {cv_score}\")\n",
    "\n",
    "# Test score\n",
    "test_score = optimal_model_gradboost.score(X_test, y_test)\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9d251",
   "metadata": {},
   "source": [
    "#### Discussion2: I initially claim that if my model's train score is greater than 0.85 then I will say there is no problem of overfitting. In this case the train score is equal to 0.9986 which is greater than my threshhold of underfitting then I claim that here this model has no issue with underfitting. Now let's move on and compare Train and CV scores. 0.9986 and 0.9259 are significantly far from each other, because I claim that if their difference less than 0.05 then the difference is significant and there is a possible issue of overfitting. As the Train and CV scores are significantly different from each other we can claim that there is an issue of overfitting. Now let's compare Test and CV scores.  0.95277 and 0.9259 are close to each other (less than the threshhold 0.05), so we can claim that the CV and Test sets are coming from the same distribution, so we are confident that the we have well tuned values for our hyperparams. Ultimatelly based on the abovementioned facts I claim that this model is not good enough because there is a possible issue of overfitting according to my threshold, so further works(for example regularization, collect more data ) to overcome this issue. So I do not accept this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1265e5",
   "metadata": {},
   "source": [
    "#### Based on my two discussion about model1(Adaboostclassifeir) and model2(gradientboostclassifier) I prefer the model1, because it performs better(has no issue with underfit, overfitting and we are confident that parameters are tuned well), but in case of model2 there is some issue with overfitting that makes me afraid to choose that model. So Model1(Adaboostclassifier) is the winner!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1eac90",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bff68",
   "metadata": {},
   "source": [
    "## My ID - A09190061, last two digits - 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89a66aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "data = make_friedman1(n_samples=2000, n_features=10, noise=1, random_state=61)\n",
    "columns = ['Feature ' + str(i) for i in range(1, 11)]\n",
    "X = pd.DataFrame(data[0])\n",
    "X.columns = columns\n",
    "y = data[1]\n",
    "\n",
    "# Splitting to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd3513b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.824469</td>\n",
       "      <td>0.181350</td>\n",
       "      <td>0.877327</td>\n",
       "      <td>0.499551</td>\n",
       "      <td>0.964926</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.923644</td>\n",
       "      <td>0.629444</td>\n",
       "      <td>0.304298</td>\n",
       "      <td>0.340925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140735</td>\n",
       "      <td>0.487002</td>\n",
       "      <td>0.881047</td>\n",
       "      <td>0.452995</td>\n",
       "      <td>0.474998</td>\n",
       "      <td>0.882341</td>\n",
       "      <td>0.667464</td>\n",
       "      <td>0.081829</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>0.720033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.711822</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>0.466009</td>\n",
       "      <td>0.367395</td>\n",
       "      <td>0.599223</td>\n",
       "      <td>0.724687</td>\n",
       "      <td>0.191987</td>\n",
       "      <td>0.528219</td>\n",
       "      <td>0.291443</td>\n",
       "      <td>0.139491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.955981</td>\n",
       "      <td>0.605044</td>\n",
       "      <td>0.910282</td>\n",
       "      <td>0.457832</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>0.126563</td>\n",
       "      <td>0.334569</td>\n",
       "      <td>0.447856</td>\n",
       "      <td>0.425973</td>\n",
       "      <td>0.548966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.769691</td>\n",
       "      <td>0.922199</td>\n",
       "      <td>0.749690</td>\n",
       "      <td>0.292347</td>\n",
       "      <td>0.193074</td>\n",
       "      <td>0.260583</td>\n",
       "      <td>0.726084</td>\n",
       "      <td>0.454452</td>\n",
       "      <td>0.171107</td>\n",
       "      <td>0.642443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.800617</td>\n",
       "      <td>0.075045</td>\n",
       "      <td>0.409141</td>\n",
       "      <td>0.398778</td>\n",
       "      <td>0.563133</td>\n",
       "      <td>0.963809</td>\n",
       "      <td>0.322028</td>\n",
       "      <td>0.954875</td>\n",
       "      <td>0.095632</td>\n",
       "      <td>0.368797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.258222</td>\n",
       "      <td>0.314044</td>\n",
       "      <td>0.262170</td>\n",
       "      <td>0.036709</td>\n",
       "      <td>0.194434</td>\n",
       "      <td>0.902738</td>\n",
       "      <td>0.937243</td>\n",
       "      <td>0.400341</td>\n",
       "      <td>0.067509</td>\n",
       "      <td>0.024034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.831863</td>\n",
       "      <td>0.905300</td>\n",
       "      <td>0.939497</td>\n",
       "      <td>0.356512</td>\n",
       "      <td>0.369527</td>\n",
       "      <td>0.788484</td>\n",
       "      <td>0.260343</td>\n",
       "      <td>0.467541</td>\n",
       "      <td>0.586687</td>\n",
       "      <td>0.427316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.536900</td>\n",
       "      <td>0.476609</td>\n",
       "      <td>0.290489</td>\n",
       "      <td>0.555933</td>\n",
       "      <td>0.955677</td>\n",
       "      <td>0.368670</td>\n",
       "      <td>0.285442</td>\n",
       "      <td>0.072169</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.506708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.431781</td>\n",
       "      <td>0.385187</td>\n",
       "      <td>0.800209</td>\n",
       "      <td>0.189079</td>\n",
       "      <td>0.239313</td>\n",
       "      <td>0.027727</td>\n",
       "      <td>0.861399</td>\n",
       "      <td>0.703168</td>\n",
       "      <td>0.628970</td>\n",
       "      <td>0.710375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  Feature 6  \\\n",
       "0      0.824469   0.181350   0.877327   0.499551   0.964926   0.003546   \n",
       "1      0.140735   0.487002   0.881047   0.452995   0.474998   0.882341   \n",
       "2      0.711822   0.039873   0.466009   0.367395   0.599223   0.724687   \n",
       "3      0.955981   0.605044   0.910282   0.457832   0.043483   0.126563   \n",
       "4      0.769691   0.922199   0.749690   0.292347   0.193074   0.260583   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1995   0.800617   0.075045   0.409141   0.398778   0.563133   0.963809   \n",
       "1996   0.258222   0.314044   0.262170   0.036709   0.194434   0.902738   \n",
       "1997   0.831863   0.905300   0.939497   0.356512   0.369527   0.788484   \n",
       "1998   0.536900   0.476609   0.290489   0.555933   0.955677   0.368670   \n",
       "1999   0.431781   0.385187   0.800209   0.189079   0.239313   0.027727   \n",
       "\n",
       "      Feature 7  Feature 8  Feature 9  Feature 10  \n",
       "0      0.923644   0.629444   0.304298    0.340925  \n",
       "1      0.667464   0.081829   0.027823    0.720033  \n",
       "2      0.191987   0.528219   0.291443    0.139491  \n",
       "3      0.334569   0.447856   0.425973    0.548966  \n",
       "4      0.726084   0.454452   0.171107    0.642443  \n",
       "...         ...        ...        ...         ...  \n",
       "1995   0.322028   0.954875   0.095632    0.368797  \n",
       "1996   0.937243   0.400341   0.067509    0.024034  \n",
       "1997   0.260343   0.467541   0.586687    0.427316  \n",
       "1998   0.285442   0.072169   0.840359    0.506708  \n",
       "1999   0.861399   0.703168   0.628970    0.710375  \n",
       "\n",
       "[2000 rows x 10 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labeled features\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0122c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scaler for the pipeline\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Pipeline for Gradient boosting\n",
    "model_gb = make_pipeline(scaler, GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18966ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('gradientboostingregressor',\n",
       "                                               GradientBoostingRegressor())]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={'gradientboostingregressor__learning_rate': [0.001,\n",
       "                                                                             0.01,\n",
       "                                                                             0.1,\n",
       "                                                                             1,\n",
       "                                                                             2],\n",
       "                                'gradientboostingregressor__min_samples_split': array([2, 3]),\n",
       "                                'gradientboostingregressor__n_estimators': array([ 50, 100, 150, 200, 250, 300])},\n",
       "                    refit=<function _refit_callable at 0x000001CFCFC31040>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning for greadient boosting\n",
    "n_estimators_grid = np.arange(50, 350, 50)\n",
    "min_samples_split_grid = np.arange(2, 4)\n",
    "learning_rate_grid = [0.001, 0.01, 0.1, 1, 2]\n",
    "\n",
    "optimal_model_gb = HalvingGridSearchCV(model_gb, \n",
    "    param_grid={\n",
    "        \"gradientboostingregressor__n_estimators\": n_estimators_grid, \n",
    "        \"gradientboostingregressor__min_samples_split\": min_samples_split_grid, \n",
    "        \"gradientboostingregressor__learning_rate\": learning_rate_grid\n",
    "    }, cv = 3,  n_jobs=-1)\n",
    "optimal_model_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4016a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'gradientboostingregressor__learning_rate': 0.1, 'gradientboostingregressor__min_samples_split': 2, 'gradientboostingregressor__n_estimators': 300}\n",
      "Best score (R2): 0.9086344279980335\n"
     ]
    }
   ],
   "source": [
    "# Obtain the optimal model's best parameters and score\n",
    "best_params = optimal_model_gb.best_params_\n",
    "best_score = optimal_model_gb.best_score_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score (R2): {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### So we found out that the optimal learning rate is 0.1, min_samples_split is 2, nestimators is 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "589c8766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 important features:\n",
      "Feature 4 - Importance: 0.32132915622927644\n",
      "Feature 1 - Importance: 0.23835610754448985\n",
      "Feature 2 - Importance: 0.23749743273388346\n",
      "Feature 3 - Importance: 0.10215900253355434\n",
      "Feature 5 - Importance: 0.08712713666186808\n"
     ]
    }
   ],
   "source": [
    "# Find the 5 most important features\n",
    "gb_clf = optimal_model_gb.best_estimator_.named_steps['gradientboostingregressor']\n",
    "importances = gb_clf.feature_importances_\n",
    "indices = np.argsort(importances)[-5:]\n",
    "\n",
    "print(\"Top 5 important features:\")\n",
    "for i in indices[::-1]:\n",
    "    print(f\"{X.columns[i]} - Importance: {importances[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfcbe02",
   "metadata": {},
   "source": [
    "#### Top 5 important features:\n",
    "#### Feature 4 - Importance: 0.32132915622927644\n",
    "#### Feature 1 - Importance: 0.23835610754448985\n",
    "#### Feature 2 - Importance: 0.23749743273388346\n",
    "#### Feature 3 - Importance: 0.10215900253355434\n",
    "#### Feature 5 - Importance: 0.08712713666186808        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27452d94",
   "metadata": {},
   "source": [
    "#### Feature 4 has the highest importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5cbb69b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.977539757680062\n",
      "Test score: 0.9247100255860646\n",
      "CV score: 0.9086344279980335\n"
     ]
    }
   ],
   "source": [
    "# Calculate R-squared scores for training and test sets\n",
    "train_score = optimal_model_gb.score(X_train, y_train)\n",
    "test_score = optimal_model_gb.score(X_test, y_test)\n",
    "\n",
    "# Print the R-squared scores\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")\n",
    "print(f\"CV score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5fa0c",
   "metadata": {},
   "source": [
    "#### Modelevaluation1: I initially claim that if my model's train score is greater than 0.85 then I will say there is no problem of overfitting. In this case the train score is equal to 0.9775 which is greater than my threshhold of underfitting then I claim that here this model has no issue with underfitting. Now let's move on and compare Train and CV scores. 0.977 and 0.908 are significantly far from each other, because I claim that if their difference less than 0.05 then the difference is significant, so eventually there is overfitting issue. As the Train and CV scores are not close to each other we can claim that there is an issue of overfitting. Now let's compare Test and CV scores.  0.924 and 0.908 are very close to each other (less than the threshhold 0.05), so we can claim that the CV and Test sets are coming from the same distribution, so we are confident that the we have well tuned values for our hyperparams. Ultimatelly based on the abovementioned facts I claim that this model is not acceptable, because of the issue of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3300b15",
   "metadata": {},
   "source": [
    "## Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37a6e10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HalvingGridSearchCV(cv=3,\n",
       "                    estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                                              ('polynomialfeatures',\n",
       "                                               PolynomialFeatures()),\n",
       "                                              ('gradientboostingregressor',\n",
       "                                               GradientBoostingRegressor())]),\n",
       "                    n_jobs=-1,\n",
       "                    param_grid={'gradientboostingregressor__learning_rate': [0.001,\n",
       "                                                                             0.01,\n",
       "                                                                             0.1,\n",
       "                                                                             1,\n",
       "                                                                             2],\n",
       "                                'gradientboostingregressor__min_samples_split': array([2, 3]),\n",
       "                                'gradientboostingregressor__n_estimators': array([ 50, 100, 150, 200, 250, 300]),\n",
       "                                'polynomialfeatures__degree': [2, 3]},\n",
       "                    refit=<function _refit_callable at 0x000001CFCFC31040>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add PolynomialFeatures to the pipeline\n",
    "model_gb_poly = make_pipeline(scaler, PolynomialFeatures(), GradientBoostingRegressor())\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting Regressor with PolynomialFeatures\n",
    "optimal_model_gb_poly = HalvingGridSearchCV(\n",
    "    model_gb_poly, \n",
    "    param_grid={\n",
    "        \"polynomialfeatures__degree\": [2, 3],\n",
    "        \"gradientboostingregressor__n_estimators\": n_estimators_grid,\n",
    "        \"gradientboostingregressor__min_samples_split\": min_samples_split_grid,\n",
    "        \"gradientboostingregressor__learning_rate\": learning_rate_grid\n",
    "    }, \n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "optimal_model_gb_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4588493b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'gradientboostingregressor__learning_rate': 0.1, 'gradientboostingregressor__min_samples_split': 2, 'gradientboostingregressor__n_estimators': 150, 'polynomialfeatures__degree': 3}\n",
      "Best score (R2): 0.9226695980572658\n"
     ]
    }
   ],
   "source": [
    "best_params_poly = optimal_model_gb_poly.best_params_\n",
    "best_score_poly = optimal_model_gb_poly.best_score_\n",
    "print(f\"Best parameters: {best_params_poly}\")\n",
    "print(f\"Best score (R2): {best_score_poly}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "66487cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  Importance\n",
      "12  Feature 1 Feature 2    0.327015\n",
      "39  Feature 4 Feature 5    0.159019\n",
      "14  Feature 1 Feature 4    0.139231\n",
      "4             Feature 4    0.072732\n",
      "23  Feature 2 Feature 4    0.068726\n"
     ]
    }
   ],
   "source": [
    "poly_feature_names = optimal_model_gb_poly.best_estimator_.steps[1][1].get_feature_names(X.columns)\n",
    "\n",
    "# Get feature importances\n",
    "importances = optimal_model_gb_poly.best_estimator_.named_steps['gradientboostingregressor'].feature_importances_\n",
    "\n",
    "# Create a dataframe with feature names and their importances\n",
    "feature_importances = pd.DataFrame({'Feature': poly_feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the features by their importance in descending order\n",
    "top_5_features = feature_importances.sort_values(by='Importance', ascending=False).head()\n",
    "\n",
    "# Print the top 5 features and their importances\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aabfb2",
   "metadata": {},
   "source": [
    "#### Here are they, the first is the artadryal of feature1 and feature2, the second one is the artadryal of Feature 4 and Feature 5, then the artadryal of Feature 1 and Feature 4, then only feature4, and finally the artadryal of Feature 2 and Feature 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9419af87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.976602087337717\n",
      "Test score: 0.938211263912042\n",
      "CV score: 0.9086344279980335\n"
     ]
    }
   ],
   "source": [
    "# Calculate R-squared scores for training and test sets\n",
    "train_score = optimal_model_gb_poly.score(X_train, y_train)\n",
    "test_score = optimal_model_gb_poly.score(X_test, y_test)\n",
    "\n",
    "# Print the R-squared scores\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")\n",
    "print(f\"CV score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737bb63",
   "metadata": {},
   "source": [
    "#### Modelevaluation2: I initially claim that if my model's train score is greater than 0.85 then I will say there is no problem of overfitting. In this case the train score is equal to 0.976 which is greater than my threshhold of underfitting then I claim that here this model has no issue with underfitting. Now let's move on and compare Train and CV scores. 0.976 and 0.9086 are significantly far from each other, because I claim that if their difference less than 0.05 then the difference is significant, so eventually there is overfitting issue. As the Train and CV scores are not close to each other we can claim that there is an issue of overfitting. Now let's compare Test and CV scores.  0.938 and 0.908 are very close to each other (less than the threshhold 0.05), so we can claim that the CV and Test sets are coming from the same distribution, so we are confident that the we have well tuned values for our hyperparams. Ultimatelly based on the abovementioned facts I claim that this model is not acceptable, because of the issue of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745f00c",
   "metadata": {},
   "source": [
    "#### Overall both models are may have issue of overfitting, and further work should be done to improve them, if one asked which one should I chose, I would say none because both have the same issue. We should make further improvements and then again compare them. But after all second model's test score is slightly higher, which may seem that it is better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
